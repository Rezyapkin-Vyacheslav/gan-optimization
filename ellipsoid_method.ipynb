{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ellipsoid_method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQY7xZd6HLde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj0dCG_YH1io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EllipsoidMethod(Optimizer):\n",
        "    \"\"\"Implements Ellipsoid Method.\"\"\"\n",
        "    def __init__(self, PARAM1, PARAM2):\n",
        "        defaults = dict(PARAM1=PARAM1, PARAM2=PARAM2)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Perform Ellipsoid Method step.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad\n",
        "                # do the step...\n",
        "        \n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}